{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the suitability filter on WILDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code used to evaluate the suitability filter on WILDS datasets (as presented in the paper). Please note that models were trained using the code in `utils/wilds_models.py` and features and split indices were pre-computed based on the code in `utils/prepare_features_and_splits/`. We include the specific features and split indices used for our experiments in `results/features/` and `results/split_indices/`, so the below can be run without training new WILDS models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ece_and_bias(probs, correct, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate the Expected Calibration Error (ECE) and Calibration Bias (CB).\n",
    "    \n",
    "    Args:\n",
    "        probs (np.ndarray): Array of predicted probabilities for the positive class, shape (n_samples,).\n",
    "        correct (np.ndarray): Array of correct binary labels (0 or 1), shape (n_samples,).\n",
    "        n_bins (int): Number of bins to use for calibration calculation.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ECE, CB), where:\n",
    "            - ECE (float): Expected Calibration Error.\n",
    "            - CB (float): Calibration Bias (positive = overestimation, negative = underestimation).\n",
    "    \"\"\"\n",
    "    # Define bin edges and initialize variables\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0\n",
    "    cb = 0\n",
    "    \n",
    "    # Assign probabilities to bins\n",
    "    bin_indices = np.digitize(probs, bins) - 1  # Map probabilities to bin indices (0 to n_bins-1)\n",
    "    \n",
    "    # Calculate ECE and CB\n",
    "    for i in range(n_bins):\n",
    "        # Mask for the current bin\n",
    "        bin_mask = bin_indices == i\n",
    "        if np.sum(bin_mask) == 0:  # Skip empty bins\n",
    "            continue\n",
    "        \n",
    "        # Bin accuracy and confidence\n",
    "        bin_accuracy = np.mean(correct[bin_mask])\n",
    "        bin_confidence = np.mean(probs[bin_mask])\n",
    "        \n",
    "        # Bin weight\n",
    "        bin_weight = np.sum(bin_mask) / len(correct)\n",
    "        \n",
    "        # Update ECE and CB\n",
    "        ece += bin_weight * np.abs(bin_accuracy - bin_confidence)\n",
    "        # cb += bin_weight * (bin_confidence - bin_accuracy)\n",
    "\n",
    "    cb = np.mean(probs) - np.mean(correct)\n",
    "    \n",
    "    return ece, cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seed = 0\n",
    "\n",
    "# Define suitability filter and experiment parameters\n",
    "classifiers = [\n",
    "    \"logistic_regression\"\n",
    "]  # [\"logistic_regression\", \"svm\", \"random_forest\", \"gradient_boosting\", \"mlp\", \"decision_tree\"]\n",
    "margins = [0] # [0, 0.005, 0.01, 0.05]\n",
    "normalize = True\n",
    "calibrated = True\n",
    "feature_subsets = [\n",
    "    # [0],\n",
    "    # [1],\n",
    "    # [2],\n",
    "    # [3],\n",
    "    # [4],\n",
    "    # [5],\n",
    "    # [6],\n",
    "    # [7],\n",
    "    # [8],\n",
    "    # [9],\n",
    "    # [10],\n",
    "    # [11],\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "]\n",
    "num_fold_arr = [15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"fmow\"\n",
    "\n",
    "# Load the features\n",
    "feature_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "with open(feature_cache_file, \"rb\") as f:\n",
    "    full_feature_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_results = []\n",
    "\n",
    "id_feature_dict = {}\n",
    "id_feature_dict[\"id_val\"] = full_feature_dict[\"id_val\"]\n",
    "id_feature_dict[\"id_test\"] = full_feature_dict[\"id_test\"]\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    id_split_dict = pickle.load(f)\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(id_split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = id_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = id_feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    # Re-partition remaining data into folds\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(all_corr)), user_split_indices)\n",
    "    remaining_features = all_features[remaining_indices]\n",
    "    remaining_corr = all_corr[remaining_indices]\n",
    "    if user_split_name == \"id_val\":\n",
    "        other_split_name = \"id_test\"\n",
    "    elif user_split_name == \"id_test\":\n",
    "        other_split_name = \"id_val\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split name: {user_split_name}\")\n",
    "    additional_features, additional_corr = id_feature_dict[other_split_name]\n",
    "    source_features = np.concatenate([remaining_features, additional_features], axis=0)\n",
    "    source_corr = np.concatenate([remaining_corr, additional_corr], axis=0)\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin, return_predictions=True\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            pred_user = sf_test[\"user_predictions\"]\n",
    "                            pred_test = sf_test[\"test_predictions\"]\n",
    "\n",
    "                            # Calculate ECE and CB\n",
    "                            ece_user, cb_user = calculate_ece_and_bias(\n",
    "                                pred_user, user_corr\n",
    "                            )\n",
    "                            ece_test, cb_test = calculate_ece_and_bias(\n",
    "                                pred_test, test_corr\n",
    "                            )\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                    \"ece_user\": ece_user,\n",
    "                                    \"cb_user\": cb_user,\n",
    "                                    \"ece_test\": ece_test,\n",
    "                                    \"cb_test\": cb_test,\n",
    "                                    \"mean_pred_user\": np.mean(pred_user),\n",
    "                                    \"mean_pred_test\": np.mean(pred_test),\n",
    "                                    \"std_pred_user\": np.std(pred_user),\n",
    "                                    \"std_pred_test\": np.std(pred_test),\n",
    "                                }\n",
    "                            )          \n",
    "\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/{data_name}_{algorithm}_{model_type}_{seed}_id.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_results = []\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    ood_split_dict = pickle.load(f)\n",
    "\n",
    "id_features_val, id_corr_val = full_feature_dict[\"id_val\"]\n",
    "id_features_test, id_corr_test = full_feature_dict[\"id_test\"]\n",
    "source_features = np.concatenate([id_features_val, id_features_test], axis=0)\n",
    "source_corr = np.concatenate([id_corr_val, id_corr_test], axis=0)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(ood_split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = ood_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = full_feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin, return_predictions=True\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            pred_user = sf_test[\"user_predictions\"]\n",
    "                            pred_test = sf_test[\"test_predictions\"]\n",
    "\n",
    "                            # Calculate ECE and CB\n",
    "                            ece_user, cb_user = calculate_ece_and_bias(\n",
    "                                pred_user, user_corr\n",
    "                            )\n",
    "                            ece_test, cb_test = calculate_ece_and_bias(\n",
    "                                pred_test, test_corr\n",
    "                            )\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                    \"ece_user\": ece_user,\n",
    "                                    \"cb_user\": cb_user,\n",
    "                                    \"ece_test\": ece_test,\n",
    "                                    \"cb_test\": cb_test,\n",
    "                                    \"mean_pred_user\": np.mean(pred_user),\n",
    "                                    \"mean_pred_test\": np.mean(pred_test),\n",
    "                                    \"std_pred_user\": np.std(pred_user),\n",
    "                                    \"std_pred_test\": np.std(pred_test),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/{data_name}_{algorithm}_{model_type}_{seed}_ood.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RXRX1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"rxrx1\"\n",
    "\n",
    "# Load the features\n",
    "feature_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "with open(feature_cache_file, \"rb\") as f:\n",
    "    full_feature_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_results = []\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    split_dict = pickle.load(f)\n",
    "\n",
    "all_keys = list(split_dict.keys())\n",
    "for key in all_keys:\n",
    "    if key[0] == 'test' or key[0] == 'val':\n",
    "        del split_dict[key]\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = full_feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    # Re-partition remaining data into folds\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(all_corr)), user_split_indices)\n",
    "    source_features = all_features[remaining_indices]\n",
    "    source_corr = all_corr[remaining_indices]\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/{data_name}_{algorithm}_{model_type}_{seed}_id.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_results = []\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    ood_split_dict = pickle.load(f)\n",
    "\n",
    "all_keys = list(ood_split_dict.keys())\n",
    "for key in all_keys:\n",
    "    if key[0] == 'id_test':\n",
    "        del ood_split_dict[key]\n",
    "\n",
    "source_features, source_corr = full_feature_dict[\"id_test\"]\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(ood_split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = ood_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = full_feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/{data_name}_{algorithm}_{model_type}_{seed}_ood.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIVILCOMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"civilcomments\"\n",
    "\n",
    "# Load the features\n",
    "feature_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "with open(feature_cache_file, \"rb\") as f:\n",
    "    feature_dict = pickle.load(f)\n",
    "\n",
    "sf_results = []\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    split_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    # Re-partition remaining data into folds\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(all_corr)), user_split_indices)\n",
    "    remaining_features = all_features[remaining_indices]\n",
    "    remaining_corr = all_corr[remaining_indices]\n",
    "    if user_split_name == \"val\":\n",
    "        other_split_name = \"test\"\n",
    "    elif user_split_name == \"test\":\n",
    "        other_split_name = \"val\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split name: {user_split_name}\")\n",
    "    additional_features, additional_corr = feature_dict[other_split_name]\n",
    "    source_features = np.concatenate([remaining_features, additional_features], axis=0)\n",
    "    source_corr = np.concatenate([remaining_corr, additional_corr], axis=0)\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/{data_name}_{algorithm}_{model_type}_{seed}_.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
