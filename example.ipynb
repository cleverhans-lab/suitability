{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce376b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from filter.suitability_filter import get_sf_features, SuitabilityFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7aec1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier SF data: 1000 samples from MNIST train\n",
      "Test SF data: 500 samples from MNIST test\n",
      "User SF data: 500 samples from MNIST test\n",
      "\n",
      "Starting SimpleCNN training on cuda...\n",
      "Epoch [1/5], Step [200/938], Loss: 0.4675\n",
      "Epoch [1/5], Step [400/938], Loss: 0.1327\n",
      "Epoch [1/5], Step [600/938], Loss: 0.0950\n",
      "Epoch [1/5], Step [800/938], Loss: 0.0906\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0622\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0569\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0618\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0511\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0427\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0442\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0410\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0435\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0326\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0343\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0339\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0324\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0255\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0259\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0269\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0328\n",
      "SimpleCNN training finished.\n",
      "\n",
      "Accuracy of the trained SimpleCNN on the 10000 test images: 99.01%\n",
      "Shape of classifier_features: (1000, 12)\n",
      "SF data correctness: 114 correct out of 1000 (approx accuracy: 0.11)\n",
      "Shape of test_features: (500, 12)\n",
      "Test correctness: 52 correct out of 500 (approx accuracy: 0.10)\n",
      "Shape of user_features: (500, 12)\n",
      "\n",
      "Suitability Test Results:\n",
      "  t_statistic: -1.1872\n",
      "  p_value: 0.1177\n",
      "  reject_null: False\n",
      "\n",
      "User data is NOT proven non-inferior (p=0.1177 >= 0.05). We cannot conclude that the new data is within the non-inferiority margin of the test data.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10  # MNIST has 10 classes (digits 0-9)\n",
    "IMG_SIZE = 28     # MNIST image size\n",
    "BATCH_SIZE = 64   # Adjusted batch size\n",
    "\n",
    "### DATA PREPARATION\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Mean and std for MNIST\n",
    "])\n",
    "\n",
    "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test_full = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create subsets for our three data groups for the suitability filter\n",
    "# classifier_loader_sf: Data to train the prediction correctness classifier\n",
    "classifier_indices_sf = list(range(0, 1000))\n",
    "classifier_dataset_sf = Subset(mnist_train_full, classifier_indices_sf)\n",
    "classifier_loader_sf = DataLoader(classifier_dataset_sf, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_indices_sf = list(range(0, 500))\n",
    "test_dataset_sf = Subset(mnist_test_full, test_indices_sf)\n",
    "test_loader_sf = DataLoader(test_dataset_sf, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "user_indices_sf = list(range(500, 1000)) # Distinct from test_dataset_sf\n",
    "user_dataset_sf = Subset(mnist_test_full, user_indices_sf)\n",
    "user_loader_sf = DataLoader(user_dataset_sf, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Classifier SF data: {len(classifier_dataset_sf)} samples from MNIST train\")\n",
    "print(f\"Test SF data: {len(test_dataset_sf)} samples from MNIST test\")\n",
    "print(f\"User SF data: {len(user_dataset_sf)} samples from MNIST test\")\n",
    "\n",
    "\n",
    "### MODEL DEFINITION\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1) # MNIST is 1 channel\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 28x28 -> 14x14\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 14x14 -> 7x7\n",
    "        self.fc = nn.Linear(32 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7) # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### MODEL TRAINING (New Section) ###\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Hyperparameters for CNN training\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5  # Number of epochs to train the CNN (can be increased for better performance)\n",
    "\n",
    "# DataLoaders for CNN training (using full datasets)\n",
    "# We use mnist_train_full for training the CNN itself\n",
    "cnn_train_loader = DataLoader(mnist_train_full, batch_size=BATCH_SIZE, shuffle=True)\n",
    "cnn_test_loader = DataLoader(mnist_test_full, batch_size=BATCH_SIZE, shuffle=False) # For evaluating the CNN\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\nStarting SimpleCNN training on {device}...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(cnn_train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 200 == 0: # Print progress every 200 batches\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(cnn_train_loader)}], Loss: {running_loss/200:.4f}\")\n",
    "            running_loss = 0.0\n",
    "print(\"SimpleCNN training finished.\")\n",
    "\n",
    "# Evaluate the trained CNN on the full test set\n",
    "model.eval() # Set model to evaluation mode\n",
    "correct_cnn = 0\n",
    "total_cnn = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in cnn_test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_cnn += labels.size(0)\n",
    "        correct_cnn += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_accuracy = 100 * correct_cnn / total_cnn\n",
    "print(f\"\\nAccuracy of the trained SimpleCNN on the {total_cnn} test images: {cnn_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "### FEATURE EXTRACTION\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "classifier_feats, classifier_corr = get_sf_features(classifier_loader_sf, model, device)\n",
    "print(f\"Shape of classifier_features: {classifier_feats.shape}\")\n",
    "print(f\"SF data correctness: {np.sum(classifier_corr)} correct out of {len(classifier_corr)} (approx accuracy: {np.mean(classifier_corr):.2f})\")\n",
    "\n",
    "test_feats, test_corr = get_sf_features(test_loader_sf, model, device)\n",
    "print(f\"Shape of test_features: {test_feats.shape}\")\n",
    "print(f\"Test correctness: {np.sum(test_corr)} correct out of {len(test_corr)} (approx accuracy: {np.mean(test_corr):.2f})\")\n",
    "\n",
    "user_feats, _ = get_sf_features(user_loader_sf, model, device)\n",
    "print(f\"Shape of user_features: {user_feats.shape}\")\n",
    "\n",
    "### SUITABILITY FILTER\n",
    "\n",
    "sf_filter = SuitabilityFilter(\n",
    "    test_features=test_feats,\n",
    "    test_corr=test_corr, # Correctness of primary model on SF's \"test\" data\n",
    "    classifier_features=classifier_feats,\n",
    "    classifier_corr=classifier_corr, # Correctness of primary model on SF's \"classifier training\" data\n",
    "    device=device,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "sf_filter.train_classifier(classifier=\"logistic_regression\", calibrated=True)\n",
    "test_margin = 0.01 \n",
    "\n",
    "results = sf_filter.suitability_test(user_features=user_feats, margin=test_margin)\n",
    "print(\"\\nSuitability Test Results:\")\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if results['p_value'] < alpha:\n",
    "    print(f\"\\nUser data IS considered non-inferior (p={results['p_value']:.4f} < {alpha}). The new data is not significantly worse than the test data by more than the margin.\")\n",
    "else:\n",
    "    print(f\"\\nUser data is NOT proven non-inferior (p={results['p_value']:.4f} >= {alpha}). We cannot conclude that the new data is within the non-inferiority margin of the test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729f63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
