{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the suitability filter on FMoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import random\n",
    "from itertools import chain, combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from suitability.datasets.wilds import get_wilds_dataset, get_wilds_model\n",
    "from suitability.filter import suitability_efficient\n",
    "\n",
    "importlib.reload(suitability_efficient)\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter, get_sf_features\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & evaluate all possible splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val_splits = [\n",
    "    (\"id_val\", {\"year\": 2002}),\n",
    "    (\"id_val\", {\"year\": 2003}),\n",
    "    (\"id_val\", {\"year\": 2004}),\n",
    "    (\"id_val\", {\"year\": 2005}),\n",
    "    (\"id_val\", {\"year\": 2006}),\n",
    "    (\"id_val\", {\"year\": 2007}),\n",
    "    (\"id_val\", {\"year\": 2008}),\n",
    "    (\"id_val\", {\"year\": 2009}),\n",
    "    (\"id_val\", {\"year\": 2010}),\n",
    "    (\"id_val\", {\"year\": 2011}),\n",
    "    (\"id_val\", {\"year\": 2012}),\n",
    "    (\"id_val\", {\"region\": \"Asia\"}),\n",
    "    (\"id_val\", {\"region\": \"Europe\"}),\n",
    "    (\"id_val\", {\"region\": \"Africa\"}),\n",
    "    (\"id_val\", {\"region\": \"Americas\"}),\n",
    "    (\"id_val\", {\"region\": \"Oceania\"}),\n",
    "]\n",
    "\n",
    "id_test_splits = [\n",
    "    (\"id_test\", {\"year\": 2002}),\n",
    "    (\"id_test\", {\"year\": 2003}),\n",
    "    (\"id_test\", {\"year\": 2004}),\n",
    "    (\"id_test\", {\"year\": 2005}),\n",
    "    (\"id_test\", {\"year\": 2006}),\n",
    "    (\"id_test\", {\"year\": 2007}),\n",
    "    (\"id_test\", {\"year\": 2008}),\n",
    "    (\"id_test\", {\"year\": 2009}),\n",
    "    (\"id_test\", {\"year\": 2010}),\n",
    "    (\"id_test\", {\"year\": 2011}),\n",
    "    (\"id_test\", {\"year\": 2012}),\n",
    "    (\"id_test\", {\"region\": \"Asia\"}),\n",
    "    (\"id_test\", {\"region\": \"Europe\"}),\n",
    "    (\"id_test\", {\"region\": \"Africa\"}),\n",
    "    (\"id_test\", {\"region\": \"Americas\"}),\n",
    "    (\"id_test\", {\"region\": \"Oceania\"}),\n",
    "]\n",
    "\n",
    "ood_val_splits = [\n",
    "    (\"val\", {\"year\": 2013}),\n",
    "    (\"val\", {\"year\": 2014}),\n",
    "    (\"val\", {\"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\"}),\n",
    "    (\"val\", {\"region\": \"Europe\"}),\n",
    "    (\"val\", {\"region\": \"Africa\"}),\n",
    "    (\"val\", {\"region\": \"Americas\"}),\n",
    "    (\"val\", {\"region\": \"Oceania\"}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "]\n",
    "\n",
    "ood_test_splits = [\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/u/apouget/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_wilds_model(data_name, root_dir, algorithm=\"ERM\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_splits = id_val_splits + id_test_splits + ood_val_splits + ood_test_splits\n",
    "\n",
    "results = pd.DataFrame(columns=[\"split\", \"year\", \"region\", \"num_samples\", \"accuracy\"])\n",
    "\n",
    "for split, pre_filter in all_splits:\n",
    "    data = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=pre_filter,\n",
    "    )\n",
    "    suitability_filter = SuitabilityFilter(model, data, data, device)\n",
    "    corr = suitability_filter.get_correct(data)\n",
    "\n",
    "    num_samples = len(data.dataset)\n",
    "    accuracy = np.mean(corr)\n",
    "    year = pre_filter.get(\"year\", \"ALL\")\n",
    "    region = pre_filter.get(\"region\", \"ALL\")\n",
    "    results = results._append(\n",
    "        {\n",
    "            \"split\": split,\n",
    "            \"year\": year,\n",
    "            \"region\": region,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"accuracy\": accuracy,\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "results.to_csv(\"suitability/results/data_splits/fmow_ERM_0_last.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate suitability filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid id splits: 16, number of valid ood splits: 30\n"
     ]
    }
   ],
   "source": [
    "valid_id_splits = [\n",
    "    (\"id_val\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_val\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_val\", {\"year\": [2010]}),\n",
    "    (\"id_val\", {\"year\": [2011]}),\n",
    "    (\"id_val\", {\"year\": [2012]}),\n",
    "    (\"id_val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"id_test\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_test\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_test\", {\"year\": [2010]}),\n",
    "    (\"id_test\", {\"year\": [2011]}),\n",
    "    (\"id_test\", {\"year\": [2012]}),\n",
    "    (\"id_test\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Americas\"]}),\n",
    "]\n",
    "\n",
    "valid_ood_splits = [\n",
    "    (\"val\", {\"year\": [2013]}),\n",
    "    (\"val\", {\"year\": [2014]}),\n",
    "    (\"val\", {\"year\": [2015]}),\n",
    "    (\"val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"val\", {\"region\": [\"Africa\"]}),\n",
    "    (\"val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"val\", {\"region\": [\"Oceania\"]}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Number of valid id splits: {len(valid_id_splits)}, number of valid ood splits: {len(valid_ood_splits)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate suitability filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from suitability.datasets.wilds import get_wilds_dataset, get_wilds_model\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter, get_sf_features\n",
    "\n",
    "cache_file_id_combined = \"suitability/results/features/fmow_ERM_last_0_combined_id.pkl\"\n",
    "if os.path.exists(cache_file_id_combined):\n",
    "    with open(cache_file_id_combined, \"rb\") as f:\n",
    "        features_id_combined = pickle.load(f)\n",
    "\n",
    "cache_file_id_individual = \"suitability/results/features/fmow_ERM_last_0_id.pkl\"\n",
    "if os.path.exists(cache_file_id_individual):\n",
    "    with open(cache_file_id_individual, \"rb\") as f:\n",
    "        features_id_individual = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99883533e-01,  1.26985088e-01,  1.36794173e-03, -1.35100746e+01,\n",
       "        4.84576941e+00,  4.60921860e+00,  1.01284866e+01,  1.16460695e-04,\n",
       "       -1.01284847e+01,  2.50463672e+04,  9.99990404e-01, -4.84588575e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_id_individual[(\"id_test\", \"{'region': ['Asia']}\")][\"features\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_id_individual[(\"id_test\", \"{'region': ['Asia']}\")][\"indices\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.94658554,   0.1202942 ,   0.21097937, -14.761689  ,\n",
       "         1.6514555 ,   4.5276346 ,   2.8811173 ,   0.05489393,\n",
       "        -2.8811173 ,  17.834188  ,   0.9999731 ,  -1.7063494 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_id_combined[\"features\"][120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mfsnic/u/apouget/envs/test/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mfsnic/u/apouget/envs/test/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to device: cuda\n",
      "Computing features for split: id_val\n",
      "Computing features for split: id_test\n",
      "Computing features for split: val\n",
      "Computing features for split: test\n",
      "ID splits features computed\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from suitability.datasets.wilds import get_wilds_dataset, get_wilds_model\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter, get_sf_features\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "\n",
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seed = 0\n",
    "model = get_wilds_model(\n",
    "    data_name, root_dir, algorithm=algorithm, seed=seed, model_type=model_type\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded to device: {device}\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "features_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "valid_splits = [\"id_val\", \"id_test\", \"val\", \"test\"]\n",
    "splits_features_cache = {}\n",
    "\n",
    "# Precompute all data features\n",
    "for split_name in valid_splits:\n",
    "    print(f\"Computing features for split: {split_name}\")\n",
    "    dataset = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    splits_features_cache[split_name] = get_sf_features(dataset, model, device)\n",
    "print(\"ID splits features computed\")\n",
    "\n",
    "# Save feature cache\n",
    "with open(features_cache_file, \"wb\") as f:\n",
    "    pickle.dump(splits_features_cache, f)\n",
    "\n",
    "# Precompute all id split indices\n",
    "id_cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "\n",
    "valid_id_splits = [\n",
    "    (\"id_val\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_val\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_val\", {\"year\": [2010]}),\n",
    "    (\"id_val\", {\"year\": [2011]}),\n",
    "    (\"id_val\", {\"year\": [2012]}),\n",
    "    (\"id_val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"id_test\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_test\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_test\", {\"year\": [2010]}),\n",
    "    (\"id_test\", {\"year\": [2011]}),\n",
    "    (\"id_test\", {\"year\": [2012]}),\n",
    "    (\"id_test\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Americas\"]}),\n",
    "]\n",
    "\n",
    "id_splits_indices_cache = {}\n",
    "for split_name, split_filter in valid_id_splits:\n",
    "    print(f\"Computing indices for split: {split_name} with filter: {split_filter}\")\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    id_splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "with open(id_cache_file, \"wb\") as f:\n",
    "    pickle.dump(id_splits_indices_cache, f)\n",
    "\n",
    "# Precompute all ood split indices\n",
    "ood_cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "\n",
    "valid_ood_splits = [\n",
    "    (\"val\", {\"year\": [2013]}),\n",
    "    (\"val\", {\"year\": [2014]}),\n",
    "    (\"val\", {\"year\": [2015]}),\n",
    "    (\"val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"val\", {\"region\": [\"Africa\"]}),\n",
    "    (\"val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"val\", {\"region\": [\"Oceania\"]}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]\n",
    "\n",
    "ood_splits_indices_cache = {}\n",
    "\n",
    "for split_name, split_filter in valid_ood_splits:\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    ood_splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "# Save cache\n",
    "with open(ood_cache_file, \"wb\") as f:\n",
    "    pickle.dump(ood_cache_file, f)\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('id_val', \"{'year': [2002, 2003, 2004, 2005, 2006]}\"), ('id_val', \"{'year': [2007, 2008, 2009]}\"), ('id_val', \"{'year': [2010]}\"), ('id_val', \"{'year': [2011]}\"), ('id_val', \"{'year': [2012]}\"), ('id_val', \"{'region': ['Asia']}\"), ('id_val', \"{'region': ['Europe']}\"), ('id_val', \"{'region': ['Americas']}\"), ('id_test', \"{'year': [2002, 2003, 2004, 2005, 2006]}\"), ('id_test', \"{'year': [2007, 2008, 2009]}\"), ('id_test', \"{'year': [2010]}\"), ('id_test', \"{'year': [2011]}\"), ('id_test', \"{'year': [2012]}\"), ('id_test', \"{'region': ['Asia']}\"), ('id_test', \"{'region': ['Europe']}\"), ('id_test', \"{'region': ['Americas']}\")])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "x = pickle.load(\n",
    "    open(\"/h/321/apouget/suitability/results/split_indices/fmow_id.pkl\", \"rb\")\n",
    ")\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420\n"
     ]
    }
   ],
   "source": [
    "for a, b in x.keys():\n",
    "    indices = x[(a, b)]\n",
    "    break\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1420, 12)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "y = pickle.load(\n",
    "    open(\"/h/321/apouget/suitability/results/features/fmow_ERM_last_0.pkl\", \"rb\")\n",
    ")\n",
    "print(y[\"id_val\"][0][indices].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing indices for split: id_val with filter: {'year': [2002, 2003, 2004, 2005, 2006]}\n",
      "Computing indices for split: id_val with filter: {'year': [2007, 2008, 2009]}\n",
      "Computing indices for split: id_val with filter: {'year': [2010]}\n",
      "Computing indices for split: id_val with filter: {'year': [2011]}\n",
      "Computing indices for split: id_val with filter: {'year': [2012]}\n",
      "Computing indices for split: id_val with filter: {'region': ['Asia']}\n",
      "Computing indices for split: id_val with filter: {'region': ['Europe']}\n",
      "Computing indices for split: id_val with filter: {'region': ['Americas']}\n",
      "Computing indices for split: id_test with filter: {'year': [2002, 2003, 2004, 2005, 2006]}\n",
      "Computing indices for split: id_test with filter: {'year': [2007, 2008, 2009]}\n",
      "Computing indices for split: id_test with filter: {'year': [2010]}\n",
      "Computing indices for split: id_test with filter: {'year': [2011]}\n",
      "Computing indices for split: id_test with filter: {'year': [2012]}\n",
      "Computing indices for split: id_test with filter: {'region': ['Asia']}\n",
      "Computing indices for split: id_test with filter: {'region': ['Europe']}\n",
      "Computing indices for split: id_test with filter: {'region': ['Americas']}\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "\n",
    "valid_id_splits = [\n",
    "    (\"id_val\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_val\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_val\", {\"year\": [2010]}),\n",
    "    (\"id_val\", {\"year\": [2011]}),\n",
    "    (\"id_val\", {\"year\": [2012]}),\n",
    "    (\"id_val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"id_test\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_test\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_test\", {\"year\": [2010]}),\n",
    "    (\"id_test\", {\"year\": [2011]}),\n",
    "    (\"id_test\", {\"year\": [2012]}),\n",
    "    (\"id_test\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Americas\"]}),\n",
    "]\n",
    "\n",
    "splits_indices_cache = {}\n",
    "\n",
    "\n",
    "# Precompute all data features\n",
    "for split_name, split_filter in valid_id_splits:\n",
    "    print(f\"Computing indices for split: {split_name} with filter: {split_filter}\")\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "# Save cache\n",
    "with open(cache_file, \"wb\") as f:\n",
    "    pickle.dump(splits_indices_cache, f)\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing indices for split: val with filter: {'year': [2013]}\n",
      "Computing indices for split: val with filter: {'year': [2014]}\n",
      "Computing indices for split: val with filter: {'year': [2015]}\n",
      "Computing indices for split: val with filter: {'region': ['Asia']}\n",
      "Computing indices for split: val with filter: {'region': ['Europe']}\n",
      "Computing indices for split: val with filter: {'region': ['Africa']}\n",
      "Computing indices for split: val with filter: {'region': ['Americas']}\n",
      "Computing indices for split: val with filter: {'region': ['Oceania']}\n",
      "Computing indices for split: val with filter: {'region': 'Europe', 'year': 2013}\n",
      "Computing indices for split: val with filter: {'region': 'Europe', 'year': 2014}\n",
      "Computing indices for split: val with filter: {'region': 'Europe', 'year': 2015}\n",
      "Computing indices for split: val with filter: {'region': 'Asia', 'year': 2013}\n",
      "Computing indices for split: val with filter: {'region': 'Asia', 'year': 2014}\n",
      "Computing indices for split: val with filter: {'region': 'Asia', 'year': 2015}\n",
      "Computing indices for split: val with filter: {'region': 'Americas', 'year': 2013}\n",
      "Computing indices for split: val with filter: {'region': 'Americas', 'year': 2014}\n",
      "Computing indices for split: val with filter: {'region': 'Americas', 'year': 2015}\n",
      "Computing indices for split: test with filter: {'year': 2016}\n",
      "Computing indices for split: test with filter: {'year': 2017}\n",
      "Computing indices for split: test with filter: {'region': 'Asia'}\n",
      "Computing indices for split: test with filter: {'region': 'Europe'}\n",
      "Computing indices for split: test with filter: {'region': 'Africa'}\n",
      "Computing indices for split: test with filter: {'region': 'Americas'}\n",
      "Computing indices for split: test with filter: {'region': 'Oceania'}\n",
      "Computing indices for split: test with filter: {'region': 'Europe', 'year': 2016}\n",
      "Computing indices for split: test with filter: {'region': 'Europe', 'year': 2017}\n",
      "Computing indices for split: test with filter: {'region': 'Asia', 'year': 2016}\n",
      "Computing indices for split: test with filter: {'region': 'Asia', 'year': 2017}\n",
      "Computing indices for split: test with filter: {'region': 'Americas', 'year': 2016}\n",
      "Computing indices for split: test with filter: {'region': 'Americas', 'year': 2017}\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "\n",
    "valid_ood_splits = [\n",
    "    (\"val\", {\"year\": [2013]}),\n",
    "    (\"val\", {\"year\": [2014]}),\n",
    "    (\"val\", {\"year\": [2015]}),\n",
    "    (\"val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"val\", {\"region\": [\"Africa\"]}),\n",
    "    (\"val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"val\", {\"region\": [\"Oceania\"]}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]\n",
    "\n",
    "splits_indices_cache = {}\n",
    "\n",
    "\n",
    "# Precompute all data features\n",
    "for split_name, split_filter in valid_ood_splits:\n",
    "    print(f\"Computing indices for split: {split_name} with filter: {split_filter}\")\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "# Save cache\n",
    "with open(cache_file, \"wb\") as f:\n",
    "    pickle.dump(splits_indices_cache, f)\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_ece_and_bias(probs, correct, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate the Expected Calibration Error (ECE) and Calibration Bias (CB).\n",
    "    \n",
    "    Args:\n",
    "        probs (np.ndarray): Array of predicted probabilities for the positive class, shape (n_samples,).\n",
    "        correct (np.ndarray): Array of correct binary labels (0 or 1), shape (n_samples,).\n",
    "        n_bins (int): Number of bins to use for calibration calculation.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ECE, CB), where:\n",
    "            - ECE (float): Expected Calibration Error.\n",
    "            - CB (float): Calibration Bias (positive = overestimation, negative = underestimation).\n",
    "    \"\"\"\n",
    "    # Define bin edges and initialize variables\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0\n",
    "    cb = 0\n",
    "    \n",
    "    # Assign probabilities to bins\n",
    "    bin_indices = np.digitize(probs, bins) - 1  # Map probabilities to bin indices (0 to n_bins-1)\n",
    "    \n",
    "    # Calculate ECE and CB\n",
    "    for i in range(n_bins):\n",
    "        # Mask for the current bin\n",
    "        bin_mask = bin_indices == i\n",
    "        if np.sum(bin_mask) == 0:  # Skip empty bins\n",
    "            continue\n",
    "        \n",
    "        # Bin accuracy and confidence\n",
    "        bin_accuracy = np.mean(correct[bin_mask])\n",
    "        bin_confidence = np.mean(probs[bin_mask])\n",
    "        \n",
    "        # Bin weight\n",
    "        bin_weight = np.sum(bin_mask) / len(correct)\n",
    "        \n",
    "        # Update ECE and CB\n",
    "        ece += bin_weight * np.abs(bin_accuracy - bin_confidence)\n",
    "        # cb += bin_weight * (bin_confidence - bin_accuracy)\n",
    "\n",
    "    cb = np.mean(probs) - np.mean(correct)\n",
    "    \n",
    "    return ece, cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID SPLIT SUBSET EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                     | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▊                                                                                                                     | 1/16 [00:12<03:08, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████████▋                                                                                                             | 2/16 [00:25<02:56, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████▍                                                                                                     | 3/16 [00:38<02:45, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████▎                                                                                             | 4/16 [00:51<02:36, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████████████████                                                                                      | 5/16 [01:04<02:23, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████▉                                                                              | 6/16 [01:17<02:10, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████▋                                                                      | 7/16 [01:31<01:58, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████▌                                                              | 8/16 [01:44<01:44, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████▎                                                      | 9/16 [01:57<01:32, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████████████████▌                                              | 10/16 [02:10<01:18, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████████████████████████████████████▎                                      | 11/16 [02:22<01:04, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████                               | 12/16 [02:35<00:51, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                       | 13/16 [02:48<00:38, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 14/16 [03:02<00:26, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 15/16 [03:15<00:13, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:28<00:00, 13.03s/it]\n",
      "  0%|                                                                                                                                     | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▊                                                                                                                     | 1/16 [00:12<03:13, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████████▋                                                                                                             | 2/16 [00:25<02:59, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████▍                                                                                                     | 3/16 [00:39<02:52, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████▎                                                                                             | 4/16 [00:52<02:37, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████████████████                                                                                      | 5/16 [01:05<02:24, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████▉                                                                              | 6/16 [01:18<02:10, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████▋                                                                      | 7/16 [01:32<02:00, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████▌                                                              | 8/16 [01:45<01:46, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████▎                                                      | 9/16 [01:58<01:31, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████████████████▌                                              | 10/16 [02:11<01:18, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████████████████████████████████████▎                                      | 11/16 [02:24<01:05, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████                               | 12/16 [02:37<00:52, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                       | 13/16 [02:51<00:39, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 14/16 [03:03<00:26, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 15/16 [03:17<00:13, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:30<00:00, 13.15s/it]\n",
      "  0%|                                                                                                                                     | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▊                                                                                                                     | 1/16 [00:13<03:19, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████████▋                                                                                                             | 2/16 [00:26<03:01, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████▍                                                                                                     | 3/16 [00:39<02:49, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████▎                                                                                             | 4/16 [00:52<02:36, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████████████████                                                                                      | 5/16 [01:05<02:23, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████▉                                                                              | 6/16 [01:18<02:11, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████▋                                                                      | 7/16 [01:32<01:59, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████▌                                                              | 8/16 [01:45<01:45, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████▎                                                      | 9/16 [01:57<01:31, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████████████████▌                                              | 10/16 [02:10<01:17, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████████████████████████████████████▎                                      | 11/16 [02:23<01:05, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████                               | 12/16 [02:36<00:52, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                       | 13/16 [02:49<00:39, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 14/16 [03:02<00:26, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 15/16 [03:16<00:13, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:29<00:00, 13.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter\n",
    "from suitability.filter.tests import non_inferiority_ttest\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "\n",
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "for seed in seeds:\n",
    "    # Load the features\n",
    "    feature_cache_file = (\n",
    "        f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    "    )\n",
    "    with open(feature_cache_file, \"rb\") as f:\n",
    "        full_feature_dict = pickle.load(f)\n",
    "    id_feature_dict = {}\n",
    "    id_feature_dict[\"id_val\"] = full_feature_dict[\"id_val\"]\n",
    "    id_feature_dict[\"id_test\"] = full_feature_dict[\"id_test\"]\n",
    "\n",
    "    # Load the split indices\n",
    "    split_cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "    with open(split_cache_file, \"rb\") as f:\n",
    "        id_split_dict = pickle.load(f)\n",
    "\n",
    "    # Define suitability filter and experiment parameters\n",
    "    classifiers = [\n",
    "        \"logistic_regression\"\n",
    "    ]  # \"logistic_regression\", \"svm\", \"random_forest\", \"gradient_boosting\", \"mlp\", \"decision_tree\"]\n",
    "    margins = [0, 0.005, 0.01, 0.05]\n",
    "    normalize = True\n",
    "    calibrated = True\n",
    "    sf_results = []\n",
    "    direct_testing_results = []\n",
    "    feature_subsets = [\n",
    "        # [0],\n",
    "        # [1],\n",
    "        # [2],\n",
    "        # [3],\n",
    "        # [4],\n",
    "        # [5],\n",
    "        # [6],\n",
    "        # [7],\n",
    "        # [8],\n",
    "        # [9],\n",
    "        # [10],\n",
    "        # [11],\n",
    "        # [4, 11],\n",
    "        # [4, 11, 8],\n",
    "        # [4, 11, 8, 6],\n",
    "        # [4, 11, 8, 6, 2],\n",
    "        # [4, 11, 8, 6, 2, 1],\n",
    "        # [4, 11, 8, 6, 2, 1, 0],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7, 3],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7, 3, 10],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7, 3, 10, 5],\n",
    "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "    ]\n",
    "    num_fold_arr = [15]\n",
    "\n",
    "    # Main loop\n",
    "    for user_split_name, user_filter in tqdm(id_split_dict.keys()):\n",
    "        print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "        # Get user split indices\n",
    "        user_split_indices = id_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "        # Get user split features and correctness\n",
    "        all_features, all_corr = id_feature_dict[user_split_name]\n",
    "        user_features = all_features[user_split_indices]\n",
    "        user_corr = all_corr[user_split_indices]\n",
    "        user_size = len(user_corr)\n",
    "        user_acc = np.mean(user_corr)\n",
    "\n",
    "        # Re-partition remaining data into folds\n",
    "        remaining_indices = np.setdiff1d(np.arange(len(all_corr)), user_split_indices)\n",
    "        remaining_features = all_features[remaining_indices]\n",
    "        remaining_corr = all_corr[remaining_indices]\n",
    "        if user_split_name == \"id_val\":\n",
    "            other_split_name = \"id_test\"\n",
    "        elif user_split_name == \"id_test\":\n",
    "            other_split_name = \"id_val\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split name: {user_split_name}\")\n",
    "        additional_features, additional_corr = id_feature_dict[other_split_name]\n",
    "        source_features = np.concatenate([remaining_features, additional_features], axis=0)\n",
    "        source_corr = np.concatenate([remaining_corr, additional_corr], axis=0)\n",
    "\n",
    "        for num_folds in num_fold_arr:\n",
    "            source_fold_size = len(source_corr) // num_folds\n",
    "            indices = np.arange(len(source_corr))\n",
    "            np.random.shuffle(indices)\n",
    "            fold_indices = [\n",
    "                indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "                for i in range(num_folds)\n",
    "            ]\n",
    "\n",
    "            for i, reg_indices in enumerate(fold_indices):\n",
    "                reg_features = source_features[reg_indices]\n",
    "                reg_corr = source_corr[reg_indices]\n",
    "                reg_size = len(reg_corr)\n",
    "                reg_acc = np.mean(reg_corr)\n",
    "\n",
    "                for j, test_indices in enumerate(fold_indices):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    test_features = source_features[test_indices]\n",
    "                    test_corr = source_corr[test_indices]\n",
    "                    test_size = len(test_corr)\n",
    "                    test_acc = np.mean(test_corr)\n",
    "\n",
    "                    for classifier in classifiers:\n",
    "                        for feature_subset in feature_subsets:\n",
    "                            suitability_filter = SuitabilityFilter(\n",
    "                                test_features,\n",
    "                                test_corr,\n",
    "                                reg_features,\n",
    "                                reg_corr,\n",
    "                                device,\n",
    "                                normalize=normalize,\n",
    "                                feature_subset=feature_subset,\n",
    "                            )\n",
    "                            suitability_filter.train_classifier(\n",
    "                                calibrated=calibrated, classifier=classifier\n",
    "                            )\n",
    "\n",
    "                            for margin in margins:\n",
    "                                # Test suitability filter\n",
    "                                sf_test = suitability_filter.suitability_test(\n",
    "                                    user_features=user_features, margin=margin, return_predictions=True\n",
    "                                )\n",
    "                                p_value = sf_test[\"p_value\"]\n",
    "                                ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                                pred_user = sf_test[\"user_predictions\"]\n",
    "                                pred_test = sf_test[\"test_predictions\"]\n",
    "\n",
    "                                # Calculate ECE and CB\n",
    "                                ece_user, cb_user = calculate_ece_and_bias(\n",
    "                                    pred_user, user_corr\n",
    "                                )\n",
    "                                ece_test, cb_test = calculate_ece_and_bias(\n",
    "                                    pred_test, test_corr\n",
    "                                )\n",
    "\n",
    "                                sf_results.append(\n",
    "                                    {\n",
    "                                        \"data_name\": data_name,\n",
    "                                        \"algorithm\": algorithm,\n",
    "                                        \"seed\": seed,\n",
    "                                        \"model_type\": model_type,\n",
    "                                        \"normalize\": normalize,\n",
    "                                        \"calibrated\": calibrated,\n",
    "                                        \"margin\": margin,\n",
    "                                        \"reg_fold\": i,\n",
    "                                        \"reg_size\": reg_size,\n",
    "                                        \"reg_acc\": reg_acc,\n",
    "                                        \"test_fold\": j,\n",
    "                                        \"test_size\": test_size,\n",
    "                                        \"test_acc\": test_acc,\n",
    "                                        \"user_split\": user_split_name,\n",
    "                                        \"user_filter\": user_filter,\n",
    "                                        \"user_size\": user_size,\n",
    "                                        \"user_acc\": user_acc,\n",
    "                                        \"p_value\": p_value,\n",
    "                                        \"ground_truth\": ground_truth,\n",
    "                                        \"classifier\": classifier,\n",
    "                                        \"feature_subset\": feature_subset,\n",
    "                                        \"acc_diff\": user_acc - test_acc,\n",
    "                                        \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                        \"ece_user\": ece_user,\n",
    "                                        \"cb_user\": cb_user,\n",
    "                                        \"ece_test\": ece_test,\n",
    "                                        \"cb_test\": cb_test,\n",
    "                                        \"mean_pred_user\": np.mean(pred_user),\n",
    "                                        \"mean_pred_test\": np.mean(pred_test),\n",
    "                                        \"std_pred_user\": np.std(pred_user),\n",
    "                                        \"std_pred_test\": np.std(pred_test),\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "                                # Run non-inferiority test on features directly\n",
    "                                # if (\n",
    "                                #     len(feature_subset) == 1\n",
    "                                #     and margin == 0\n",
    "                                #     and classifier == \"logistic_regression\"\n",
    "                                #     and (j == 0 or (i == 0 and j == 1))\n",
    "                                # ):\n",
    "                                #     test_feature_subset = test_features[:, feature_subset].flatten()\n",
    "                                #     user_feature_subset = user_features[:, feature_subset].flatten()\n",
    "                                #     test_1 = non_inferiority_ttest(\n",
    "                                #         test_feature_subset,\n",
    "                                #         user_feature_subset,\n",
    "                                #         increase_good=True,\n",
    "                                #     )\n",
    "                                #     test_2 = non_inferiority_ttest(\n",
    "                                #         test_feature_subset,\n",
    "                                #         user_feature_subset,\n",
    "                                #         increase_good=False,\n",
    "                                #     )\n",
    "                                #     direct_testing_results.append(\n",
    "                                #         {\n",
    "                                #             \"data_name\": data_name,\n",
    "                                #             \"algorithm\": algorithm,\n",
    "                                #             \"seed\": seed,\n",
    "                                #             \"model_type\": model_type,\n",
    "                                #             \"test_fold\": j,\n",
    "                                #             \"test_size\": test_size,\n",
    "                                #             \"test_acc\": test_acc,\n",
    "                                #             \"user_split\": user_split_name,\n",
    "                                #             \"user_filter\": user_filter,\n",
    "                                #             \"user_size\": user_size,\n",
    "                                #             \"user_acc\": user_acc,\n",
    "                                #             \"p_value_increase_good\": test_1[\"p_value\"],\n",
    "                                #             \"p_value_decrease_good\": test_2[\"p_value\"],\n",
    "                                #             \"ground_truth\": ground_truth,\n",
    "                                #             \"feature_subset\": feature_subset,\n",
    "                                #             \"acc_diff\": user_acc - test_acc,\n",
    "                                #         }\n",
    "                                #     )\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    sf_evals = pd.DataFrame(sf_results)\n",
    "    sf_evals.to_csv(\n",
    "        f\"suitability/results/sf_evals/irm/fmow_sf_results_id_calibration_{algorithm}_{model_type}_{seed}_FINAL.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    # direct_testing_evals = pd.DataFrame(direct_testing_results)\n",
    "    # direct_testing_evals.to_csv(\n",
    "    #     f\"suitability/results/sf_evals/irm/fmow_direct_testing_results_id_calibration_{algorithm}_{model_type}_{seed}_NEW.csv\",\n",
    "    #     index=False,\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD SPLIT SUBSET EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                     | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'year': [2013]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter\n",
    "from suitability.filter.tests import non_inferiority_ttest\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "\n",
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "for seed in seeds:\n",
    "    # Load the features\n",
    "    feature_cache_file = (\n",
    "        f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    "    )\n",
    "    with open(feature_cache_file, \"rb\") as f:\n",
    "        full_feature_dict = pickle.load(f)\n",
    "\n",
    "    # Load the split indices\n",
    "    split_cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "    with open(split_cache_file, \"rb\") as f:\n",
    "        ood_split_dict = pickle.load(f)\n",
    "\n",
    "    # Define suitability filter and experiment parameters\n",
    "    classifiers = [\n",
    "        \"logistic_regression\"\n",
    "    ]  # \"logistic_regression\", \"svm\", \"random_forest\", \"gradient_boosting\", \"mlp\", \"decision_tree\"]\n",
    "    margins = [0, 0.005, 0.01, 0.05] #  0.005, 0.01, 0.05\n",
    "    normalize = True\n",
    "    calibrated = True\n",
    "    sf_results = []\n",
    "    direct_testing_results = []\n",
    "    feature_subsets = [\n",
    "        # [0],\n",
    "        # [1],\n",
    "        # [2],\n",
    "        # [3],\n",
    "        # [4],\n",
    "        # [5],\n",
    "        # [6],\n",
    "        # [7],\n",
    "        # [8],\n",
    "        # [9],\n",
    "        # [10],\n",
    "        # [11],\n",
    "        # [4, 11],\n",
    "        # [4, 11, 8],\n",
    "        # [4, 11, 8, 6],\n",
    "        # [4, 11, 8, 6, 2],\n",
    "        # [4, 11, 8, 6, 2, 1],\n",
    "        # [4, 11, 8, 6, 2, 1, 0],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7, 3],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7, 3, 10],\n",
    "        # [4, 11, 8, 6, 2, 1, 0, 7, 3, 10, 5],\n",
    "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "    ]\n",
    "    num_fold_arr = [15]\n",
    "\n",
    "    id_features_val, id_corr_val = full_feature_dict[\"id_val\"]\n",
    "    id_features_test, id_corr_test = full_feature_dict[\"id_test\"]\n",
    "    source_features = np.concatenate([id_features_val, id_features_test], axis=0)\n",
    "    source_corr = np.concatenate([id_corr_val, id_corr_test], axis=0)\n",
    "\n",
    "\n",
    "    # Main loop\n",
    "    for user_split_name, user_filter in tqdm(ood_split_dict.keys()):\n",
    "        print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "        # Get user split indices\n",
    "        user_split_indices = ood_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "        # Get user split features and correctness\n",
    "        all_features, all_corr = full_feature_dict[user_split_name]\n",
    "        user_features = all_features[user_split_indices]\n",
    "        user_corr = all_corr[user_split_indices]\n",
    "        user_size = len(user_corr)\n",
    "        user_acc = np.mean(user_corr)\n",
    "\n",
    "        for num_folds in num_fold_arr:\n",
    "            source_fold_size = len(source_corr) // num_folds\n",
    "            indices = np.arange(len(source_corr))\n",
    "            np.random.shuffle(indices)\n",
    "            fold_indices = [\n",
    "                indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "                for i in range(num_folds)\n",
    "            ]\n",
    "\n",
    "            for i, reg_indices in enumerate(fold_indices):\n",
    "                reg_features = source_features[reg_indices]\n",
    "                reg_corr = source_corr[reg_indices]\n",
    "                reg_size = len(reg_corr)\n",
    "                reg_acc = np.mean(reg_corr)\n",
    "\n",
    "                for j, test_indices in enumerate(fold_indices):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    test_features = source_features[test_indices]\n",
    "                    test_corr = source_corr[test_indices]\n",
    "                    test_size = len(test_corr)\n",
    "                    test_acc = np.mean(test_corr)\n",
    "\n",
    "                    for classifier in classifiers:\n",
    "                        for feature_subset in feature_subsets:\n",
    "                            suitability_filter = SuitabilityFilter(\n",
    "                                test_features,\n",
    "                                test_corr,\n",
    "                                reg_features,\n",
    "                                reg_corr,\n",
    "                                device,\n",
    "                                normalize=normalize,\n",
    "                                feature_subset=feature_subset,\n",
    "                            )\n",
    "                            suitability_filter.train_classifier(\n",
    "                                calibrated=calibrated, classifier=classifier\n",
    "                            )\n",
    "\n",
    "                            for margin in margins:\n",
    "                                # Test suitability filter\n",
    "                                sf_test = suitability_filter.suitability_test(\n",
    "                                    user_features=user_features, margin=margin, return_predictions=True\n",
    "                                )\n",
    "                                p_value = sf_test[\"p_value\"]\n",
    "                                ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                                pred_user = sf_test[\"user_predictions\"]\n",
    "                                pred_test = sf_test[\"test_predictions\"]\n",
    "\n",
    "\n",
    "                                # MARGIN ADJUSTMENT PART\n",
    "                                n_cal_samples = 50 # Number of samples for calibration error estimation\n",
    "\n",
    "                                # Ensure n_cal_samples is not larger than available data\n",
    "                                n_cal_test = min(n_cal_samples, test_size)\n",
    "                                n_cal_user = min(n_cal_samples, user_size)\n",
    "\n",
    "                                if n_cal_test > 0 and n_cal_user > 0:\n",
    "                                    # Sample indices for calibration estimation\n",
    "                                    cal_test_indices = np.random.choice(test_size, n_cal_test, replace=False)\n",
    "                                    cal_user_indices = np.random.choice(user_size, n_cal_user, replace=False)\n",
    "\n",
    "                                    # Get corresponding predictions and ground truth correctness\n",
    "                                    pred_test_cal = pred_test[cal_test_indices]\n",
    "                                    test_corr_cal = test_corr[cal_test_indices] # Index into original test_corr using sampled indices from test_indices\n",
    "\n",
    "                                    pred_user_cal = pred_user[cal_user_indices]\n",
    "                                    user_corr_cal = user_corr[cal_user_indices] # Index into original user_corr\n",
    "\n",
    "                                    # Calculate calibration errors (Delta)\n",
    "                                    delta_test = np.mean(pred_test_cal) - np.mean(test_corr_cal)\n",
    "                                    delta_u = np.mean(pred_user_cal) - np.mean(user_corr_cal)\n",
    "\n",
    "                                    # Calculate adjusted margin\n",
    "                                    m_prime = margin + delta_test - delta_u\n",
    "\n",
    "                                sf_test_adjusted = suitability_filter.suitability_test(\n",
    "                                    user_features=user_features, margin=m_prime, return_predictions=True\n",
    "                                )\n",
    "\n",
    "                                p_value_ma = sf_test_adjusted[\"p_value\"]\n",
    "\n",
    "                                # END MARGIN ADJUSTMENT PART\n",
    "\n",
    "                                sf_results.append(\n",
    "                                    {\n",
    "                                        \"data_name\": data_name,\n",
    "                                        \"algorithm\": algorithm,\n",
    "                                        \"seed\": seed,\n",
    "                                        \"model_type\": model_type,\n",
    "                                        \"normalize\": normalize,\n",
    "                                        \"calibrated\": calibrated,\n",
    "                                        \"margin\": margin,\n",
    "                                        \"reg_fold\": i,\n",
    "                                        \"reg_size\": reg_size,\n",
    "                                        \"reg_acc\": reg_acc,\n",
    "                                        \"test_fold\": j,\n",
    "                                        \"test_size\": test_size,\n",
    "                                        \"test_acc\": test_acc,\n",
    "                                        \"user_split\": user_split_name,\n",
    "                                        \"user_filter\": user_filter,\n",
    "                                        \"user_size\": user_size,\n",
    "                                        \"user_acc\": user_acc,\n",
    "                                        \"p_value\": p_value,\n",
    "                                        \"ground_truth\": ground_truth,\n",
    "                                        \"classifier\": classifier,\n",
    "                                        \"feature_subset\": feature_subset,\n",
    "                                        \"acc_diff\": user_acc - test_acc,\n",
    "                                        \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                        \"mean_pred_user\": np.mean(pred_user),\n",
    "                                        \"mean_pred_test\": np.mean(pred_test),\n",
    "                                        \"std_pred_user\": np.std(pred_user),\n",
    "                                        \"std_pred_test\": np.std(pred_test),\n",
    "                                        \"p_value_ma\": p_value_ma,\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    sf_evals = pd.DataFrame(sf_results)\n",
    "    sf_evals.to_csv(\n",
    "        f\"suitability/results/sf_evals/erm/fmow_sf_results_ood_calibration_{algorithm}_{model_type}_{seed}_REBUTTAL.csv\",\n",
    "        index=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
