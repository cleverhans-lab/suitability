{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the suitability filter on FMoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import random\n",
    "from itertools import chain, combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from suitability.datasets.wilds import get_wilds_dataset, get_wilds_model\n",
    "from suitability.filter import suitability_efficient\n",
    "\n",
    "importlib.reload(suitability_efficient)\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter, get_sf_features\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & evaluate all possible splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val_splits = [\n",
    "    (\"id_val\", {\"year\": 2002}),\n",
    "    (\"id_val\", {\"year\": 2003}),\n",
    "    (\"id_val\", {\"year\": 2004}),\n",
    "    (\"id_val\", {\"year\": 2005}),\n",
    "    (\"id_val\", {\"year\": 2006}),\n",
    "    (\"id_val\", {\"year\": 2007}),\n",
    "    (\"id_val\", {\"year\": 2008}),\n",
    "    (\"id_val\", {\"year\": 2009}),\n",
    "    (\"id_val\", {\"year\": 2010}),\n",
    "    (\"id_val\", {\"year\": 2011}),\n",
    "    (\"id_val\", {\"year\": 2012}),\n",
    "    (\"id_val\", {\"region\": \"Asia\"}),\n",
    "    (\"id_val\", {\"region\": \"Europe\"}),\n",
    "    (\"id_val\", {\"region\": \"Africa\"}),\n",
    "    (\"id_val\", {\"region\": \"Americas\"}),\n",
    "    (\"id_val\", {\"region\": \"Oceania\"}),\n",
    "]\n",
    "\n",
    "id_test_splits = [\n",
    "    (\"id_test\", {\"year\": 2002}),\n",
    "    (\"id_test\", {\"year\": 2003}),\n",
    "    (\"id_test\", {\"year\": 2004}),\n",
    "    (\"id_test\", {\"year\": 2005}),\n",
    "    (\"id_test\", {\"year\": 2006}),\n",
    "    (\"id_test\", {\"year\": 2007}),\n",
    "    (\"id_test\", {\"year\": 2008}),\n",
    "    (\"id_test\", {\"year\": 2009}),\n",
    "    (\"id_test\", {\"year\": 2010}),\n",
    "    (\"id_test\", {\"year\": 2011}),\n",
    "    (\"id_test\", {\"year\": 2012}),\n",
    "    (\"id_test\", {\"region\": \"Asia\"}),\n",
    "    (\"id_test\", {\"region\": \"Europe\"}),\n",
    "    (\"id_test\", {\"region\": \"Africa\"}),\n",
    "    (\"id_test\", {\"region\": \"Americas\"}),\n",
    "    (\"id_test\", {\"region\": \"Oceania\"}),\n",
    "]\n",
    "\n",
    "ood_val_splits = [\n",
    "    (\"val\", {\"year\": 2013}),\n",
    "    (\"val\", {\"year\": 2014}),\n",
    "    (\"val\", {\"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\"}),\n",
    "    (\"val\", {\"region\": \"Europe\"}),\n",
    "    (\"val\", {\"region\": \"Africa\"}),\n",
    "    (\"val\", {\"region\": \"Americas\"}),\n",
    "    (\"val\", {\"region\": \"Oceania\"}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "]\n",
    "\n",
    "ood_test_splits = [\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/u/apouget/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_wilds_model(data_name, root_dir, algorithm=\"ERM\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_splits = id_val_splits + id_test_splits + ood_val_splits + ood_test_splits\n",
    "\n",
    "results = pd.DataFrame(columns=[\"split\", \"year\", \"region\", \"num_samples\", \"accuracy\"])\n",
    "\n",
    "for split, pre_filter in all_splits:\n",
    "    data = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=pre_filter,\n",
    "    )\n",
    "    suitability_filter = SuitabilityFilter(model, data, data, device)\n",
    "    corr = suitability_filter.get_correct(data)\n",
    "\n",
    "    num_samples = len(data.dataset)\n",
    "    accuracy = np.mean(corr)\n",
    "    year = pre_filter.get(\"year\", \"ALL\")\n",
    "    region = pre_filter.get(\"region\", \"ALL\")\n",
    "\n",
    "    results = results._append(\n",
    "        {\n",
    "            \"split\": split,\n",
    "            \"year\": year,\n",
    "            \"region\": region,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"accuracy\": accuracy,\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "results.to_csv(\"suitability/results/data_splits/fmow_ERM_0_last.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate suitability filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid id splits: 16, number of valid ood splits: 30\n"
     ]
    }
   ],
   "source": [
    "valid_id_splits = [\n",
    "    (\"id_val\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_val\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_val\", {\"year\": [2010]}),\n",
    "    (\"id_val\", {\"year\": [2011]}),\n",
    "    (\"id_val\", {\"year\": [2012]}),\n",
    "    (\"id_val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"id_test\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_test\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_test\", {\"year\": [2010]}),\n",
    "    (\"id_test\", {\"year\": [2011]}),\n",
    "    (\"id_test\", {\"year\": [2012]}),\n",
    "    (\"id_test\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Americas\"]}),\n",
    "]\n",
    "\n",
    "valid_ood_splits = [\n",
    "    (\"val\", {\"year\": [2013]}),\n",
    "    (\"val\", {\"year\": [2014]}),\n",
    "    (\"val\", {\"year\": [2015]}),\n",
    "    (\"val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"val\", {\"region\": [\"Africa\"]}),\n",
    "    (\"val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"val\", {\"region\": [\"Oceania\"]}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Number of valid id splits: {len(valid_id_splits)}, number of valid ood splits: {len(valid_ood_splits)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate suitability filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from suitability.datasets.wilds import get_wilds_dataset, get_wilds_model\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter, get_sf_features\n",
    "\n",
    "cache_file_id_combined = \"suitability/results/features/fmow_ERM_last_0_combined_id.pkl\"\n",
    "if os.path.exists(cache_file_id_combined):\n",
    "    with open(cache_file_id_combined, \"rb\") as f:\n",
    "        features_id_combined = pickle.load(f)\n",
    "\n",
    "cache_file_id_individual = \"suitability/results/features/fmow_ERM_last_0_id.pkl\"\n",
    "if os.path.exists(cache_file_id_individual):\n",
    "    with open(cache_file_id_individual, \"rb\") as f:\n",
    "        features_id_individual = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99883533e-01,  1.26985088e-01,  1.36794173e-03, -1.35100746e+01,\n",
       "        4.84576941e+00,  4.60921860e+00,  1.01284866e+01,  1.16460695e-04,\n",
       "       -1.01284847e+01,  2.50463672e+04,  9.99990404e-01, -4.84588575e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_id_individual[(\"id_test\", \"{'region': ['Asia']}\")][\"features\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_id_individual[(\"id_test\", \"{'region': ['Asia']}\")][\"indices\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.94658554,   0.1202942 ,   0.21097937, -14.761689  ,\n",
       "         1.6514555 ,   4.5276346 ,   2.8811173 ,   0.05489393,\n",
       "        -2.8811173 ,  17.834188  ,   0.9999731 ,  -1.7063494 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_id_combined[\"features\"][120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mfsnic/u/apouget/envs/test/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mfsnic/u/apouget/envs/test/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to device: cuda\n",
      "Computing features for split: id_val\n",
      "Computing features for split: id_test\n",
      "Computing features for split: val\n",
      "Computing features for split: test\n",
      "ID splits features computed\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from suitability.datasets.wilds import get_wilds_dataset, get_wilds_model\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter, get_sf_features\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "\n",
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seed = 0\n",
    "model = get_wilds_model(\n",
    "    data_name, root_dir, algorithm=algorithm, seed=seed, model_type=model_type\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded to device: {device}\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "features_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "valid_splits = [\"id_val\", \"id_test\", \"val\", \"test\"]\n",
    "splits_features_cache = {}\n",
    "\n",
    "# Precompute all data features\n",
    "for split_name in valid_splits:\n",
    "    print(f\"Computing features for split: {split_name}\")\n",
    "    dataset = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    splits_features_cache[split_name] = get_sf_features(dataset, model, device)\n",
    "print(\"ID splits features computed\")\n",
    "\n",
    "# Save feature cache\n",
    "with open(features_cache_file, \"wb\") as f:\n",
    "    pickle.dump(splits_features_cache, f)\n",
    "\n",
    "# Precompute all id split indices\n",
    "id_cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "\n",
    "valid_id_splits = [\n",
    "    (\"id_val\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_val\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_val\", {\"year\": [2010]}),\n",
    "    (\"id_val\", {\"year\": [2011]}),\n",
    "    (\"id_val\", {\"year\": [2012]}),\n",
    "    (\"id_val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"id_test\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_test\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_test\", {\"year\": [2010]}),\n",
    "    (\"id_test\", {\"year\": [2011]}),\n",
    "    (\"id_test\", {\"year\": [2012]}),\n",
    "    (\"id_test\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Americas\"]}),\n",
    "]\n",
    "\n",
    "id_splits_indices_cache = {}\n",
    "for split_name, split_filter in valid_id_splits:\n",
    "    print(f\"Computing indices for split: {split_name} with filter: {split_filter}\")\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    id_splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "with open(id_cache_file, \"wb\") as f:\n",
    "    pickle.dump(id_splits_indices_cache, f)\n",
    "\n",
    "# Precompute all ood split indices\n",
    "ood_cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "\n",
    "valid_ood_splits = [\n",
    "    (\"val\", {\"year\": [2013]}),\n",
    "    (\"val\", {\"year\": [2014]}),\n",
    "    (\"val\", {\"year\": [2015]}),\n",
    "    (\"val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"val\", {\"region\": [\"Africa\"]}),\n",
    "    (\"val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"val\", {\"region\": [\"Oceania\"]}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]\n",
    "\n",
    "ood_splits_indices_cache = {}\n",
    "\n",
    "for split_name, split_filter in valid_ood_splits:\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    ood_splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "# Save cache\n",
    "with open(ood_cache_file, \"wb\") as f:\n",
    "    pickle.dump(ood_cache_file, f)\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('id_val', \"{'year': [2002, 2003, 2004, 2005, 2006]}\"), ('id_val', \"{'year': [2007, 2008, 2009]}\"), ('id_val', \"{'year': [2010]}\"), ('id_val', \"{'year': [2011]}\"), ('id_val', \"{'year': [2012]}\"), ('id_val', \"{'region': ['Asia']}\"), ('id_val', \"{'region': ['Europe']}\"), ('id_val', \"{'region': ['Americas']}\"), ('id_test', \"{'year': [2002, 2003, 2004, 2005, 2006]}\"), ('id_test', \"{'year': [2007, 2008, 2009]}\"), ('id_test', \"{'year': [2010]}\"), ('id_test', \"{'year': [2011]}\"), ('id_test', \"{'year': [2012]}\"), ('id_test', \"{'region': ['Asia']}\"), ('id_test', \"{'region': ['Europe']}\"), ('id_test', \"{'region': ['Americas']}\")])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "x = pickle.load(\n",
    "    open(\"/h/321/apouget/suitability/results/split_indices/fmow_id.pkl\", \"rb\")\n",
    ")\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420\n"
     ]
    }
   ],
   "source": [
    "for a, b in x.keys():\n",
    "    indices = x[(a, b)]\n",
    "    break\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1420, 12)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "y = pickle.load(\n",
    "    open(\"/h/321/apouget/suitability/results/features/fmow_ERM_last_0.pkl\", \"rb\")\n",
    ")\n",
    "print(y[\"id_val\"][0][indices].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing indices for split: id_val with filter: {'year': [2002, 2003, 2004, 2005, 2006]}\n",
      "Computing indices for split: id_val with filter: {'year': [2007, 2008, 2009]}\n",
      "Computing indices for split: id_val with filter: {'year': [2010]}\n",
      "Computing indices for split: id_val with filter: {'year': [2011]}\n",
      "Computing indices for split: id_val with filter: {'year': [2012]}\n",
      "Computing indices for split: id_val with filter: {'region': ['Asia']}\n",
      "Computing indices for split: id_val with filter: {'region': ['Europe']}\n",
      "Computing indices for split: id_val with filter: {'region': ['Americas']}\n",
      "Computing indices for split: id_test with filter: {'year': [2002, 2003, 2004, 2005, 2006]}\n",
      "Computing indices for split: id_test with filter: {'year': [2007, 2008, 2009]}\n",
      "Computing indices for split: id_test with filter: {'year': [2010]}\n",
      "Computing indices for split: id_test with filter: {'year': [2011]}\n",
      "Computing indices for split: id_test with filter: {'year': [2012]}\n",
      "Computing indices for split: id_test with filter: {'region': ['Asia']}\n",
      "Computing indices for split: id_test with filter: {'region': ['Europe']}\n",
      "Computing indices for split: id_test with filter: {'region': ['Americas']}\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "\n",
    "valid_id_splits = [\n",
    "    (\"id_val\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_val\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_val\", {\"year\": [2010]}),\n",
    "    (\"id_val\", {\"year\": [2011]}),\n",
    "    (\"id_val\", {\"year\": [2012]}),\n",
    "    (\"id_val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"id_test\", {\"year\": [2002, 2003, 2004, 2005, 2006]}),\n",
    "    (\"id_test\", {\"year\": [2007, 2008, 2009]}),\n",
    "    (\"id_test\", {\"year\": [2010]}),\n",
    "    (\"id_test\", {\"year\": [2011]}),\n",
    "    (\"id_test\", {\"year\": [2012]}),\n",
    "    (\"id_test\", {\"region\": [\"Asia\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Europe\"]}),\n",
    "    (\"id_test\", {\"region\": [\"Americas\"]}),\n",
    "]\n",
    "\n",
    "splits_indices_cache = {}\n",
    "\n",
    "\n",
    "# Precompute all data features\n",
    "for split_name, split_filter in valid_id_splits:\n",
    "    print(f\"Computing indices for split: {split_name} with filter: {split_filter}\")\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "# Save cache\n",
    "with open(cache_file, \"wb\") as f:\n",
    "    pickle.dump(splits_indices_cache, f)\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing indices for split: val with filter: {'year': [2013]}\n",
      "Computing indices for split: val with filter: {'year': [2014]}\n",
      "Computing indices for split: val with filter: {'year': [2015]}\n",
      "Computing indices for split: val with filter: {'region': ['Asia']}\n",
      "Computing indices for split: val with filter: {'region': ['Europe']}\n",
      "Computing indices for split: val with filter: {'region': ['Africa']}\n",
      "Computing indices for split: val with filter: {'region': ['Americas']}\n",
      "Computing indices for split: val with filter: {'region': ['Oceania']}\n",
      "Computing indices for split: val with filter: {'region': 'Europe', 'year': 2013}\n",
      "Computing indices for split: val with filter: {'region': 'Europe', 'year': 2014}\n",
      "Computing indices for split: val with filter: {'region': 'Europe', 'year': 2015}\n",
      "Computing indices for split: val with filter: {'region': 'Asia', 'year': 2013}\n",
      "Computing indices for split: val with filter: {'region': 'Asia', 'year': 2014}\n",
      "Computing indices for split: val with filter: {'region': 'Asia', 'year': 2015}\n",
      "Computing indices for split: val with filter: {'region': 'Americas', 'year': 2013}\n",
      "Computing indices for split: val with filter: {'region': 'Americas', 'year': 2014}\n",
      "Computing indices for split: val with filter: {'region': 'Americas', 'year': 2015}\n",
      "Computing indices for split: test with filter: {'year': 2016}\n",
      "Computing indices for split: test with filter: {'year': 2017}\n",
      "Computing indices for split: test with filter: {'region': 'Asia'}\n",
      "Computing indices for split: test with filter: {'region': 'Europe'}\n",
      "Computing indices for split: test with filter: {'region': 'Africa'}\n",
      "Computing indices for split: test with filter: {'region': 'Americas'}\n",
      "Computing indices for split: test with filter: {'region': 'Oceania'}\n",
      "Computing indices for split: test with filter: {'region': 'Europe', 'year': 2016}\n",
      "Computing indices for split: test with filter: {'region': 'Europe', 'year': 2017}\n",
      "Computing indices for split: test with filter: {'region': 'Asia', 'year': 2016}\n",
      "Computing indices for split: test with filter: {'region': 'Asia', 'year': 2017}\n",
      "Computing indices for split: test with filter: {'region': 'Americas', 'year': 2016}\n",
      "Computing indices for split: test with filter: {'region': 'Americas', 'year': 2017}\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "\n",
    "valid_ood_splits = [\n",
    "    (\"val\", {\"year\": [2013]}),\n",
    "    (\"val\", {\"year\": [2014]}),\n",
    "    (\"val\", {\"year\": [2015]}),\n",
    "    (\"val\", {\"region\": [\"Asia\"]}),\n",
    "    (\"val\", {\"region\": [\"Europe\"]}),\n",
    "    (\"val\", {\"region\": [\"Africa\"]}),\n",
    "    (\"val\", {\"region\": [\"Americas\"]}),\n",
    "    (\"val\", {\"region\": [\"Oceania\"]}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Europe\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Asia\", \"year\": 2015}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2013}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2014}),\n",
    "    (\"val\", {\"region\": \"Americas\", \"year\": 2015}),\n",
    "    (\"test\", {\"year\": 2016}),\n",
    "    (\"test\", {\"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\"}),\n",
    "    (\"test\", {\"region\": \"Europe\"}),\n",
    "    (\"test\", {\"region\": \"Africa\"}),\n",
    "    (\"test\", {\"region\": \"Americas\"}),\n",
    "    (\"test\", {\"region\": \"Oceania\"}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Europe\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Asia\", \"year\": 2017}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2016}),\n",
    "    (\"test\", {\"region\": \"Americas\", \"year\": 2017}),\n",
    "]\n",
    "\n",
    "splits_indices_cache = {}\n",
    "\n",
    "\n",
    "# Precompute all data features\n",
    "for split_name, split_filter in valid_ood_splits:\n",
    "    print(f\"Computing indices for split: {split_name} with filter: {split_filter}\")\n",
    "    dataset, indices = get_wilds_dataset(\n",
    "        data_name,\n",
    "        root_dir,\n",
    "        split_name,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pre_filter=split_filter,\n",
    "        return_indices=True,\n",
    "    )\n",
    "    splits_indices_cache[(split_name, str(split_filter))] = indices\n",
    "\n",
    "# Save cache\n",
    "with open(cache_file, \"wb\") as f:\n",
    "    pickle.dump(splits_indices_cache, f)\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID SPLIT SUBSET EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                      | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████▋                                                                                                    | 1/16 [05:17<1:19:25, 317.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████▍                                                                                             | 2/16 [10:20<1:12:08, 309.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████████                                                                                       | 3/16 [15:32<1:07:14, 310.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████▊                                                                                | 4/16 [20:46<1:02:23, 311.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|██████████████████████████████████                                                                           | 5/16 [26:04<57:35, 314.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████▉                                                                    | 6/16 [31:18<52:18, 313.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████▋                                                             | 7/16 [36:43<47:37, 317.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_val with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████▌                                                      | 8/16 [42:01<42:20, 317.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2002, 2003, 2004, 2005, 2006]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████████████████████▎                                               | 9/16 [47:03<36:30, 312.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2007, 2008, 2009]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████▌                                        | 10/16 [52:05<30:56, 309.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2010]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████████████████████████████████████████████████████████████▎                                 | 11/16 [57:17<25:51, 310.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2011]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████████▌                          | 12/16 [1:02:31<20:45, 311.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'year': [2012]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████████████████████████████████████████████████████████▏                   | 13/16 [1:07:48<15:39, 313.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████▊             | 14/16 [1:13:00<10:25, 312.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████▍      | 15/16 [1:18:25<05:16, 316.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: id_test with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [1:23:43<00:00, 313.94s/it]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter\n",
    "from suitability.filter.tests import non_inferiority_ttest\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "\n",
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seed = 1\n",
    "\n",
    "# Load the features\n",
    "feature_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "with open(feature_cache_file, \"rb\") as f:\n",
    "    full_feature_dict = pickle.load(f)\n",
    "id_feature_dict = {}\n",
    "id_feature_dict[\"id_val\"] = full_feature_dict[\"id_val\"]\n",
    "id_feature_dict[\"id_test\"] = full_feature_dict[\"id_test\"]\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}_id.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    id_split_dict = pickle.load(f)\n",
    "\n",
    "# Define suitability filter and experiment parameters\n",
    "classifiers = [\n",
    "    \"logistic_regression\"\n",
    "]  # \"logistic_regression\", \"svm\", \"random_forest\", \"gradient_boosting\", \"mlp\", \"decision_tree\"]\n",
    "margins = [0, 0.005, 0.01, 0.05]\n",
    "normalize = True\n",
    "calibrated = True\n",
    "sf_results = []\n",
    "direct_testing_results = []\n",
    "feature_subsets = [\n",
    "    [0],\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    [5],\n",
    "    [6],\n",
    "    [7],\n",
    "    [8],\n",
    "    [9],\n",
    "    [10],\n",
    "    [11],\n",
    "    [4, 11],\n",
    "    [4, 11, 8],\n",
    "    [4, 11, 8, 6],\n",
    "    [4, 11, 8, 6, 2],\n",
    "    [4, 11, 8, 6, 2, 1],\n",
    "    [4, 11, 8, 6, 2, 1, 0],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7, 3],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7, 3, 10],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7, 3, 10, 5],\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "]\n",
    "num_fold_arr = [15, 10, 5]\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(id_split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = id_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = id_feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    # Re-partition remaining data into folds\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(all_corr)), user_split_indices)\n",
    "    remaining_features = all_features[remaining_indices]\n",
    "    remaining_corr = all_corr[remaining_indices]\n",
    "    if user_split_name == \"id_val\":\n",
    "        other_split_name = \"id_test\"\n",
    "    elif user_split_name == \"id_test\":\n",
    "        other_split_name = \"id_val\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split name: {user_split_name}\")\n",
    "    additional_features, additional_corr = id_feature_dict[other_split_name]\n",
    "    source_features = np.concatenate([remaining_features, additional_features], axis=0)\n",
    "    source_corr = np.concatenate([remaining_corr, additional_corr], axis=0)\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            # Run non-inferiority test on features directly\n",
    "                            if (\n",
    "                                len(feature_subset) == 1\n",
    "                                and margin == 0\n",
    "                                and classifier == \"logistic_regression\"\n",
    "                                and (j == 0 or (i == 0 and j == 1))\n",
    "                            ):\n",
    "                                test_feature_subset = test_features[:, feature_subset].flatten()\n",
    "                                user_feature_subset = user_features[:, feature_subset].flatten()\n",
    "                                test_1 = non_inferiority_ttest(\n",
    "                                    test_feature_subset,\n",
    "                                    user_feature_subset,\n",
    "                                    increase_good=True,\n",
    "                                )\n",
    "                                test_2 = non_inferiority_ttest(\n",
    "                                    test_feature_subset,\n",
    "                                    user_feature_subset,\n",
    "                                    increase_good=False,\n",
    "                                )\n",
    "                                direct_testing_results.append(\n",
    "                                    {\n",
    "                                        \"data_name\": data_name,\n",
    "                                        \"algorithm\": algorithm,\n",
    "                                        \"seed\": seed,\n",
    "                                        \"model_type\": model_type,\n",
    "                                        \"test_fold\": j,\n",
    "                                        \"test_size\": test_size,\n",
    "                                        \"test_acc\": test_acc,\n",
    "                                        \"user_split\": user_split_name,\n",
    "                                        \"user_filter\": user_filter,\n",
    "                                        \"user_size\": user_size,\n",
    "                                        \"user_acc\": user_acc,\n",
    "                                        \"p_value_increase_good\": test_1[\"p_value\"],\n",
    "                                        \"p_value_decrease_good\": test_2[\"p_value\"],\n",
    "                                        \"ground_truth\": ground_truth,\n",
    "                                        \"feature_subset\": feature_subset,\n",
    "                                        \"acc_diff\": user_acc - test_acc,\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/fmow_sf_results_id_subsets_{algorithm}_{model_type}_{seed}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "direct_testing_evals = pd.DataFrame(direct_testing_results)\n",
    "direct_testing_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/fmow_direct_testing_results_id_subsets_{algorithm}_{model_type}_{seed}.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD SPLIT SUBSET EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                      | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'year': [2013]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▌                                                                                                       | 1/30 [05:38<2:43:44, 338.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'year': [2014]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▏                                                                                                   | 2/30 [11:21<2:39:11, 341.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'year': [2015]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▋                                                                                                | 3/30 [17:16<2:36:18, 347.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': ['Asia']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████▎                                                                                            | 4/30 [22:42<2:26:58, 339.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': ['Europe']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████▊                                                                                         | 5/30 [28:29<2:22:25, 341.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': ['Africa']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▍                                                                                     | 6/30 [33:31<2:11:21, 328.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': ['Americas']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████▉                                                                                  | 7/30 [39:12<2:07:28, 332.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': ['Oceania']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████▌                                                                              | 8/30 [44:13<1:58:12, 322.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Europe', 'year': 2013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████                                                                           | 9/30 [49:20<1:51:10, 317.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Europe', 'year': 2014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████▎                                                                      | 10/30 [54:38<1:45:52, 317.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Europe', 'year': 2015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████████████▏                                                                 | 11/30 [1:00:04<1:41:24, 320.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Asia', 'year': 2013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████▌                                                              | 12/30 [1:05:07<1:34:27, 314.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Asia', 'year': 2014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████████████████                                                           | 13/30 [1:10:10<1:28:14, 311.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Asia', 'year': 2015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████▌                                                       | 14/30 [1:15:21<1:23:00, 311.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Americas', 'year': 2013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████                                                    | 15/30 [1:20:26<1:17:18, 309.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Americas', 'year': 2014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████▍                                                | 16/30 [1:25:37<1:12:18, 309.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: val with filter {'region': 'Americas', 'year': 2015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████████████████▉                                             | 17/30 [1:31:01<1:08:02, 314.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'year': 2016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████▍                                         | 18/30 [1:37:17<1:06:32, 332.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'year': 2017}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████▊                                      | 19/30 [1:42:56<1:01:20, 334.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Asia'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████████▋                                   | 20/30 [1:48:31<55:48, 334.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Europe'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████▏                               | 21/30 [1:54:09<50:22, 335.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Africa'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████████████████████▋                            | 22/30 [1:59:27<44:02, 330.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Americas'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████████████████████████▎                        | 23/30 [2:05:13<39:05, 335.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Oceania'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████▊                     | 24/30 [2:10:14<32:29, 324.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Europe', 'year': 2016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████▎                 | 25/30 [2:15:48<27:17, 327.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Europe', 'year': 2017}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████▊              | 26/30 [2:20:53<21:22, 320.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Asia', 'year': 2016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████▍          | 27/30 [2:26:15<16:03, 321.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Asia', 'year': 2017}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|██████████████████████████████████████████████████████████████████████████████████████████████████▉       | 28/30 [2:31:25<10:35, 317.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Americas', 'year': 2016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 29/30 [2:37:04<05:24, 324.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user split: test with filter {'region': 'Americas', 'year': 2017}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [2:42:16<00:00, 324.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from suitability.filter.suitability_efficient import SuitabilityFilter\n",
    "from suitability.filter.tests import non_inferiority_ttest\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "\n",
    "# Configuration\n",
    "data_name = \"fmow\"\n",
    "root_dir = \"/mfsnic/projects/suitability/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "algorithm = \"ERM\"\n",
    "model_type = \"last\"\n",
    "seed = 1\n",
    "\n",
    "# Load the features\n",
    "feature_cache_file = (\n",
    "    f\"suitability/results/features/{data_name}_{algorithm}_{model_type}_{seed}.pkl\"\n",
    ")\n",
    "with open(feature_cache_file, \"rb\") as f:\n",
    "    full_feature_dict = pickle.load(f)\n",
    "\n",
    "# Load the split indices\n",
    "split_cache_file = f\"suitability/results/split_indices/{data_name}_ood.pkl\"\n",
    "with open(split_cache_file, \"rb\") as f:\n",
    "    ood_split_dict = pickle.load(f)\n",
    "\n",
    "# Define suitability filter and experiment parameters\n",
    "classifiers = [\n",
    "    \"logistic_regression\"\n",
    "]  # \"logistic_regression\", \"svm\", \"random_forest\", \"gradient_boosting\", \"mlp\", \"decision_tree\"]\n",
    "margins = [0, 0.005, 0.01, 0.05]\n",
    "normalize = True\n",
    "calibrated = True\n",
    "sf_results = []\n",
    "direct_testing_results = []\n",
    "feature_subsets = [\n",
    "    [0],\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    [5],\n",
    "    [6],\n",
    "    [7],\n",
    "    [8],\n",
    "    [9],\n",
    "    [10],\n",
    "    [11],\n",
    "    [4, 11],\n",
    "    [4, 11, 8],\n",
    "    [4, 11, 8, 6],\n",
    "    [4, 11, 8, 6, 2],\n",
    "    [4, 11, 8, 6, 2, 1],\n",
    "    [4, 11, 8, 6, 2, 1, 0],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7, 3],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7, 3, 10],\n",
    "    [4, 11, 8, 6, 2, 1, 0, 7, 3, 10, 5],\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "]\n",
    "num_fold_arr = [15, 10, 5]\n",
    "\n",
    "id_features_val, id_corr_val = full_feature_dict[\"id_val\"]\n",
    "id_features_test, id_corr_test = full_feature_dict[\"id_test\"]\n",
    "source_features = np.concatenate([id_features_val, id_features_test], axis=0)\n",
    "source_corr = np.concatenate([id_corr_val, id_corr_test], axis=0)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for user_split_name, user_filter in tqdm(ood_split_dict.keys()):\n",
    "    print(f\"Evaluating user split: {user_split_name} with filter {user_filter}\")\n",
    "\n",
    "    # Get user split indices\n",
    "    user_split_indices = ood_split_dict[(user_split_name, user_filter)]\n",
    "\n",
    "    # Get user split features and correctness\n",
    "    all_features, all_corr = full_feature_dict[user_split_name]\n",
    "    user_features = all_features[user_split_indices]\n",
    "    user_corr = all_corr[user_split_indices]\n",
    "    user_size = len(user_corr)\n",
    "    user_acc = np.mean(user_corr)\n",
    "\n",
    "    for num_folds in num_fold_arr:\n",
    "        source_fold_size = len(source_corr) // num_folds\n",
    "        indices = np.arange(len(source_corr))\n",
    "        np.random.shuffle(indices)\n",
    "        fold_indices = [\n",
    "            indices[i * source_fold_size : (i + 1) * source_fold_size]\n",
    "            for i in range(num_folds)\n",
    "        ]\n",
    "\n",
    "        for i, reg_indices in enumerate(fold_indices):\n",
    "            reg_features = source_features[reg_indices]\n",
    "            reg_corr = source_corr[reg_indices]\n",
    "            reg_size = len(reg_corr)\n",
    "            reg_acc = np.mean(reg_corr)\n",
    "\n",
    "            for j, test_indices in enumerate(fold_indices):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                test_features = source_features[test_indices]\n",
    "                test_corr = source_corr[test_indices]\n",
    "                test_size = len(test_corr)\n",
    "                test_acc = np.mean(test_corr)\n",
    "\n",
    "                for classifier in classifiers:\n",
    "                    for feature_subset in feature_subsets:\n",
    "                        suitability_filter = SuitabilityFilter(\n",
    "                            test_features,\n",
    "                            test_corr,\n",
    "                            reg_features,\n",
    "                            reg_corr,\n",
    "                            device,\n",
    "                            normalize=normalize,\n",
    "                            feature_subset=feature_subset,\n",
    "                        )\n",
    "                        suitability_filter.train_classifier(\n",
    "                            calibrated=calibrated, classifier=classifier\n",
    "                        )\n",
    "\n",
    "                        for margin in margins:\n",
    "                            # Test suitability filter\n",
    "                            sf_test = suitability_filter.suitability_test(\n",
    "                                user_features=user_features, margin=margin\n",
    "                            )\n",
    "                            p_value = sf_test[\"p_value\"]\n",
    "                            ground_truth = user_acc >= test_acc - margin\n",
    "\n",
    "                            sf_results.append(\n",
    "                                {\n",
    "                                    \"data_name\": data_name,\n",
    "                                    \"algorithm\": algorithm,\n",
    "                                    \"seed\": seed,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"normalize\": normalize,\n",
    "                                    \"calibrated\": calibrated,\n",
    "                                    \"margin\": margin,\n",
    "                                    \"reg_fold\": i,\n",
    "                                    \"reg_size\": reg_size,\n",
    "                                    \"reg_acc\": reg_acc,\n",
    "                                    \"test_fold\": j,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"test_acc\": test_acc,\n",
    "                                    \"user_split\": user_split_name,\n",
    "                                    \"user_filter\": user_filter,\n",
    "                                    \"user_size\": user_size,\n",
    "                                    \"user_acc\": user_acc,\n",
    "                                    \"p_value\": p_value,\n",
    "                                    \"ground_truth\": ground_truth,\n",
    "                                    \"classifier\": classifier,\n",
    "                                    \"feature_subset\": feature_subset,\n",
    "                                    \"acc_diff\": user_acc - test_acc,\n",
    "                                    \"acc_diff_adjusted\": user_acc + margin - test_acc,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            # Run non-inferiority test on features directly\n",
    "                            if (\n",
    "                                len(feature_subset) == 1\n",
    "                                and margin == 0\n",
    "                                and classifier == \"logistic_regression\"\n",
    "                                and (j == 0 or (i == 0 and j == 1))\n",
    "                            ):\n",
    "                                test_feature_subset = test_features[:, feature_subset].flatten()\n",
    "                                user_feature_subset = user_features[:, feature_subset].flatten()\n",
    "                                test_1 = non_inferiority_ttest(\n",
    "                                    test_feature_subset,\n",
    "                                    user_feature_subset,\n",
    "                                    increase_good=True,\n",
    "                                )\n",
    "                                test_2 = non_inferiority_ttest(\n",
    "                                    test_feature_subset,\n",
    "                                    user_feature_subset,\n",
    "                                    increase_good=False,\n",
    "                                )\n",
    "                                direct_testing_results.append(\n",
    "                                    {\n",
    "                                        \"data_name\": data_name,\n",
    "                                        \"algorithm\": algorithm,\n",
    "                                        \"seed\": seed,\n",
    "                                        \"model_type\": model_type,\n",
    "                                        \"test_fold\": j,\n",
    "                                        \"test_size\": test_size,\n",
    "                                        \"test_acc\": test_acc,\n",
    "                                        \"user_split\": user_split_name,\n",
    "                                        \"user_filter\": user_filter,\n",
    "                                        \"user_size\": user_size,\n",
    "                                        \"user_acc\": user_acc,\n",
    "                                        \"p_value_increase_good\": test_1[\"p_value\"],\n",
    "                                        \"p_value_decrease_good\": test_2[\"p_value\"],\n",
    "                                        \"ground_truth\": ground_truth,\n",
    "                                        \"feature_subset\": feature_subset,\n",
    "                                        \"acc_diff\": user_acc - test_acc,\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "# Save results\n",
    "sf_evals = pd.DataFrame(sf_results)\n",
    "sf_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/fmow_sf_results_ood_subsets_{algorithm}_{model_type}_{seed}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "direct_testing_evals = pd.DataFrame(direct_testing_results)\n",
    "direct_testing_evals.to_csv(\n",
    "    f\"suitability/results/sf_evals/erm/fmow_direct_testing_results_ood_subsets_{algorithm}_{model_type}_{seed}.csv\",\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
